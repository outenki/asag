from multiprocessing import Pool
import matplotlib.pyplot as plt
plt.switch_backend('agg')

import os
import argparse
import pickle
from utils import check_c_path, generate_training_test_data_f
import logging
from eva import RMSE, weightedF1, QWK, conf_mat, plot_confusion_matrix
from sklearn.svm import SVR
import numpy as np
import operator

PARSER = argparse.ArgumentParser()
PARSER.add_argument('-k', '--kernel', dest='kernel', type=str, metavar='Kernel', required=False, default='linear', help="Kernel for SVR (linear|poly|rbf|sigmoid|precomputed), linear as default")
PARSER.add_argument('-p', '--penalty', dest='penalty', type=float, metavar='Penalty pamrameter', required=False, default=1.0, help="Penalty parameter C of error term, 1.0 as default")
PARSER.add_argument('-fp', '--feature_path', dest='feature_path', type=str, metavar="Path where TSV file of feature is", required=True, help="The patn name is appointed as param for gen_feature.py. The filename needs to be appointed by -fn") 
PARSER.add_argument('-fn', '--feature_name', dest='feature_name', type=str, metavar='Feature Name', required=True, help='name of feature stored in feature_path. (bow|)')
PARSER.add_argument('-vn', '--vocab_name', dest='vocab_name', type=str, metavar='Vocab File Name', required=False, default='vocab', help='name of vocab file. "vocab" as default')
PARSER.add_argument('-e', '--epsilon', dest='epsilon', type=float, metavar="Epsilon in the epsilon-SVR model", required=False, default=0.1, help="The feature file generated by gen_feature.py, 0.1 as default") 
PARSER.add_argument('-o', '--output', dest='path_out', type=str, metavar="Output Path", required=True, help="Path to output") 
PARSER.add_argument('-m', '--multiprocess', dest='multi_proc', type=int, metavar='Multi Processing', required=True, help='Number of multiprocessing')
PARSER.add_argument('-tr', '--train_ratio', dest='train_ratio', type=float, metavar='Train Ratio', required=False, default=0.8, help='Ratio of traing data. 0.8 as default, meaning 80% of data will be used as training data')
PARSER.add_argument('-nm', '--normalize', dest='normalize', type=bool, metavar='Normalize', required=False, default=True, help='Flat if clip the predicted labels. If TRUE is set, the predicted labels will be normalized to a range form true_label_min to true_label_max. Set as TRUE as default')
PARSER.add_argument('-ri', '--rint', dest='rint', type=bool, metavar='Rounded Int', required=False, default=False, help='The way to convert float pred to int labels. If TRUE is set, np.rint will be used instead of np.astype(int). False as default.')
PARSER.add_argument('-ts', '--training_scale', dest='training_scale', type=int, metavar='Training Scale ', required=False, default=0, help='Training scale. 0 as default, that all the training data will be used.')
ARGS = PARSER.parse_args()

check_c_path(ARGS.path_out)
logging.basicConfig(
    filename='%s/svr.log' % ARGS.path_out,
    level=logging.INFO,
    format='%(levelname)s:%(asctime)s:%(message)s'
    )

LOGGER = logging.getLogger(__name__)

def run_svr(kernel, file_data, f_vocab, path_out, que_id, train_ratio=0.8, epsilon=0.1, penalty=1.0, normalize=False):
    print("processing %s" % que_id)
    path_out = '%s/%s' % (path_out, que_id)
    check_c_path(path_out)

    LOGGER.info('Q{}:Initialize SVR ...'.format(que_id))
    svr = SVR(C=penalty, epsilon=epsilon, kernel=kernel)
    LOGGER.info('Q{}:Initialize training data and test data ...'.format(que_id))
    data_train, data_test = generate_training_test_data_f(file_data, train_ratio=train_ratio)

    # read vocab pickle file
    LOGGER.info('Q{}:Reading vocab file ...'.format(que_id))
    with open(f_vocab, 'rb') as fv:
        vocab = pickle.load(fv)
    LOGGER.info('Q{}:Reading vocab file ... DONE'.format(que_id))
    items = sorted(vocab.items(), key=operator.itemgetter(1))
    tokens = list(zip(*items))[0]
    LOGGER.info('Q{}:Size of vocab : {}'.format(que_id, len(items)))
    with open('{}/tokens'.format(path_out), 'w') as ft:
        for t in tokens:
            ft.write('{}\n'.format(t))
    LOGGER.info('Q{}:Tokens :{}'.format(que_id, tokens[:10]))

    # get features and scores from training data
    scale = len(data_train)
    if ARGS.training_scale > 0:
        scale = ARGS.training_scale
    X = np.array(list(map(lambda r:r.split(','), data_train[:,3]))).astype(float)[:scale]
    y = data_train[:,2][:scale]

    LOGGER.info('Q{}:Training SVR ...'.format(que_id))
    LOGGER.info('Q{}:Training scale: {}'.format(que_id, len(X)))
    svr.fit(X, y)

    # get features from test data
    LOGGER.info('Q{}:Predicting test data ...'.format(que_id))
    X = np.array(list(map(lambda r:r.split(','), data_test[:,3]))).astype(float)
    y = svr.predict(X)
    score = data_test[:,2].astype(float)
    if normalize:
        v_max, v_min = max(score), min(score)
        y = y.clip(v_min, v_max)
    abs_diff = np.abs(y - score)
    results = np.column_stack((data_test[:,[0,1,2]], y.astype(str), abs_diff))

    # read weights of each n-gram token
    weight_token = svr.coef_
    LOGGER.info('Q{}:Weights :{}'.format(que_id, weight_token.astype(str)))
    item_weight = list(zip(tokens, weight_token[0]))

    LOGGER.info('Q{}:Output results ...'.format(que_id))
    f_pred = '%s/pred.txt' % path_out
    f_weight = '%s/weight.txt' % path_out
    f_eval = '%s/eval.txt' % path_out
    LOGGER.info('Q{}:\tOutput pred to: {}'.format(que_id, f_pred))
    LOGGER.info('Q{}:\tOutput weights to: {}'.format(que_id, f_weight))
    title = 'AnswerID\tQuetionID\tScore\tAnswer\tPredict\tAbsDiff\n'
    np.savetxt(f_pred, results, fmt='%s', delimiter='\t', newline='\n', header=title, footer='', comments='# ')
    LOGGER.info('Q{}:\tOutput pred to: {} DONE!'.format(que_id, f_pred))
    title = 'Token\tWeight\n'
    np.savetxt(f_weight, item_weight, fmt='%s', delimiter='\t', newline='\n', header=title, footer='', comments='# ')
    LOGGER.info('Q{}:\tOutput weights to: {} DONE!'.format(que_id, f_weight))
    
    # Evaluation with RMSE, QWK and wF1 score
    LOGGER.info('Q{}:\tOutput evaluation to: {}'.format(que_id, f_eval))
    score_float = data_test[:,2].astype(float)
    pred_float = y
    if ARGS.rint:
        score_int = np.rint(score_float)
        pred_int = np.rint(y)
    else:
        score_int = score_float.astype(int)
        pred_int = y.astype(int)

    LOGGER.info('Q{}:\tTrue labels: {}'.format(que_id, set(score_int)))
    LOGGER.info('Q{}:\tPredicted labels: {}'.format(que_id, set(pred_int)))

    # Generate confusion matrix
    LOGGER.info('Q{}:\tGenerating confusion matrix'.format(que_id))
    cm = conf_mat(score_int, pred_int)
    f_cm = '%s/cm.txt' % path_out
    np.savetxt(f_cm, cm, fmt='%d', delimiter='\t')
    print('max:', np.max(score_int))
    classes = list(range(int(np.max(score_int))+1))
    print('classes2:', classes)
    save_path = '%s/cm.png' % path_out
    print('1')
    LOGGER.info('Q{}:\tConfusion_matrix: {}'.format(que_id, save_path))
    plot_confusion_matrix(cm, classes, save_path, normalize=False,
                            title='Confusion matrix', cmap=plt.cm.Blues)
    print('2')
    save_path = '%s/cm_nm.png' % path_out
    LOGGER.info('Q{}:\tNormalized confusion_matrix: {}'.format(que_id, save_path))
    plot_confusion_matrix(cm, classes, save_path, normalize=True, 
                            title='Confusion matrix', cmap=plt.cm.Blues)
    print('3')



    rmse = RMSE(score_float, pred_float)
    LOGGER.info('Q{}:\trmse: {}'.format(que_id, rmse))
    qwk = QWK(score_int, pred_int)
    LOGGER.info('Q{}:\tqwk: {}'.format(que_id, qwk))
    wf1 = weightedF1(score_int, pred_int)
    LOGGER.info('Q{}:\twf1: {}'.format(que_id, wf1))
    LOGGER.info('Q{}: QWK: {}, RMSE: {}, wF1: {}'.format(que_id, qwk, rmse, wf1))
    with open(f_eval, 'w') as fe:
        fe.write('RMSE\t {}\n'.format(rmse))
        fe.write('QWK\t {}\n'.format(qwk))
        fe.write('wF1\t {}\n'.format(wf1))
    LOGGER.info('Q{}: Done.'.format(que_id))

# run the script

def run_svr_on_list(qids):
    print(qids)
    for qid in qids:
        if not os.path.isdir('%s/%s' % (feature_path, qid)):
            LOGGER.info('Q{}/{} is not dir. Skip it.'.format(feature_path, qid))
            continue
        file_data = '%s/%s/%s' % (feature_path, qid, feature_name)
        file_vocab = '%s/%s/%s' % (feature_path, qid, vocab_name)
        run_svr(ARGS.kernel, file_data, file_vocab, ARGS.path_out, qid, ARGS.train_ratio, ARGS.epsilon, ARGS.penalty, ARGS.normalize)
  
if __name__ == '__main__':
    feature_path = ARGS.feature_path
    feature_name = ARGS.feature_name
    vocab_name = ARGS.vocab_name
    que_ids = sorted(list(os.listdir(feature_path)))
    size_que = len(que_ids)
    size_step = int(size_que / ARGS.multi_proc)
    print('size_que:', size_que)
    print('multi_proc:', ARGS.multi_proc)

    pool = Pool()
    print(que_ids)
    for i in range(ARGS.multi_proc):
        que_list = que_ids[i::ARGS.multi_proc]
        pool.apply_async(run_svr_on_list, args=(que_list, ))
    pool.close()
    pool.join()

